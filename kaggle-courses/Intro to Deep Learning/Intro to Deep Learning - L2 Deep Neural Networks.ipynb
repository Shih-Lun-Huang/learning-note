{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a89d6878",
      "metadata": {
        "papermill": {
          "duration": 0.004449,
          "end_time": "2021-11-09T00:10:51.828123",
          "exception": false,
          "start_time": "2021-11-09T00:10:51.823674",
          "status": "completed"
        },
        "tags": [],
        "id": "a89d6878"
      },
      "source": [
        "# 介紹\n",
        "\n",
        "在本課中，我們將了解如何構建能夠學習深度神經網絡著名的複雜關係類型的神經網絡。\n",
        "\n",
        "這裡的關鍵思想是*模塊化(modularity)*，從更簡單的特徵單元構建一個複雜的網絡。我們已經看到了線性單元如何計算線性函數 —— 現在我們將看到如何組合和修改這些單個單元來模擬更複雜的關係。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 層(Layers)\n",
        "\n",
        "神經網絡通常將其神經元組織成**層**。當我們將具有一組公共輸入的線性單元收集在一起時，我們會得到一個 **dense** 層。\n",
        "\n",
        "<figure style=\"padding: 1em;\">\n",
        "<img src=\"https://i.imgur.com/2MA4iMV.png\" width=\"300\" alt=\"輸入層中的三個圓圈堆疊，連接到密集層中的兩個圓圈。\">\n",
        "<figcaption style=\"textalign: center; font-style: italic\"><center>兩個線性單元的密集層，接收兩個輸入和一個偏差。\n",
        "</center></figcaption>\n",
        "</figure>\n",
        "\n",
        "我們可以將神經網絡中的每一層視為執行某種相對簡單的轉換。通過深層堆疊，神經網絡可以以越來越複雜的方式轉換其輸入。在訓練有素的神經網絡中，每一層都是一個轉換，讓我們更接近解決方案。\n",
        "\n",
        "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #000000; padding: 1em; margin:24px;\">\n",
        "    <strong>多種層</strong><br>\n",
        "Keras 中的“層”是一種非常通用的東西。本質上，層可以是任何類型的<em>數據轉換</em>。許多層，例如 <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\">卷積</a> 和 <a href=\"https://www. tensorflow.org/api_docs/python/tf/keras/layers/RNN\">循環</a>層，通過使用神經元轉換數據，主要區別在於它們形成的連接模式。其他雖然用於 <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\">特徵工程</a> 或只是 <a href=\"https:// www.tensorflow.org/api_docs/python/tf/keras/layers/Add\">簡單算術</a>。有很多層可供探索——<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">了解一下</a>！\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "uiCTbYnXfZAJ"
      },
      "id": "uiCTbYnXfZAJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 激活函數\n",
        "\n",
        "然而，事實證明，中間沒有任何東西的兩個密集層本身並不比單個密集層好。**密集的層本身永遠無法將我們帶出線和平面的世界**。我們需要的是*非線性*。我們需要的是激活函數。\n",
        "\n",
        "<figure style=\"padding: 1em;\">\n",
        "<img src=\"https://i.imgur.com/OLSUEYT.png\" width=\"400\" alt=\" \">\n",
        "<figcaption style=\"textalign: center; font-style: italic\"><center>沒有激活函數，神經網絡只能學習線性關係。為了擬合曲線，我們需要使用激活函數。\n",
        "</center></figcaption>\n",
        "</figure>\n",
        "\n",
        "**激活函數**只是我們應用於每個層的輸出（它的*激活*）的一些函數。最常見的是 *rectifier* 函數 $max(0, x)$。\n",
        "\n",
        "<figure style=\"padding: 1em;\">\n",
        "<img src=\"https://i.imgur.com/aeIyAlF.png\" width=\"400\" alt=\"整流函數圖。線 y=x when x>0 and y=0 when x< 0，製作像'_/'這樣的'鉸鏈'形狀。\">\n",
        "<figcaption style=\"textalign: center; font-style: italic\"><center>\n",
        "</center></figcaption>\n",
        "</figure>\n",
        "\n",
        "rectifier 函數有一個圖形，它是一條負部分“整流(rectified)”為零的線。將該函數應用於神經元的輸出將使數據產生*彎曲*，使我們遠離簡單的線條。\n",
        "\n",
        "當我們將整流器連接到線性單元時，我們得到一個 **rectified linear unit** 或 **ReLU**。 （出於這個原因，通常將 rectifier 函數稱為“ReLU 函數”。）將 ReLU 激活應用於線性單元意味著輸出變為 `max(0, w * x + b)`，我們可以將其繪製為圖表如：\n",
        "\n",
        "<figure style=\"padding: 1em;\">\n",
        "<img src=\"https://i.imgur.com/eFry7Yu.png\" width=\"400\" alt=\"單個 ReLU 的圖。就像一個線性單元，但我們現在有一個鉸鏈(hinge shape)'_/'。\">\n",
        "<figcaption style=\"textalign: center; font-style: italic\"><center>校正線性單位。\n",
        "</center></figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "-ew8-gvaf1cg"
      },
      "id": "-ew8-gvaf1cg"
    },
    {
      "cell_type": "markdown",
      "id": "bc93b218",
      "metadata": {
        "papermill": {
          "duration": 0.003135,
          "end_time": "2021-11-09T00:10:51.834862",
          "exception": false,
          "start_time": "2021-11-09T00:10:51.831727",
          "status": "completed"
        },
        "tags": [],
        "id": "bc93b218"
      },
      "source": [
        "# 堆疊密集層(Stacking Dense Layers) #\n",
        "\n",
        "現在我們有了一些非線性，讓我們看看如何堆疊層來獲得複雜的數據轉換。\n",
        "\n",
        "<figure style=\"padding: 1em;\">\n",
        "<img src=\"https://i.imgur.com/Y5iwFQZ.png\" width=\"450\" alt=\"一個輸入層、兩個隱藏層和一個最終線性層。\">\n",
        "<figcaption style=\"textalign: center; font-style: italic\"><center>一疊密集層構成一個“全連接”網絡。\n",
        "</center></figcaption>\n",
        "</figure>\n",
        "\n",
        "**輸出層之前的層**有時被稱為 **hidden** 因為我們從來沒有直接看到它們的輸出。\n",
        "\n",
        "現在，請注意最終（輸出）層是一個**線性單元**（意思是，沒有激活函數）。這使得這個網絡適用於迴歸任務，我們試圖預測一些任意數值。其他任務（如分類）可能**需要輸出激活函數**。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立序列模型\n",
        "\n",
        "我們一直使用的`序列`模型將按照從第一到最後的序列連接一系列層：第一層獲取輸入，最後一層產生輸出。這將創建上圖中的模型："
      ],
      "metadata": {
        "id": "zt6w3I5lgMJX"
      },
      "id": "zt6w3I5lgMJX"
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKn_44H0gUUq",
        "outputId": "76cf739b-96ff-4e48-ae28-d16f5489c763"
      },
      "id": "TKn_44H0gUUq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8ce37c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-09T00:10:51.845197Z",
          "iopub.status.busy": "2021-11-09T00:10:51.844159Z",
          "iopub.status.idle": "2021-11-09T00:10:58.096915Z",
          "shell.execute_reply": "2021-11-09T00:10:58.097417Z"
        },
        "papermill": {
          "duration": 6.259561,
          "end_time": "2021-11-09T00:10:58.097696",
          "exception": false,
          "start_time": "2021-11-09T00:10:51.838135",
          "status": "completed"
        },
        "tags": [],
        "id": "3f8ce37c"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # 隱藏層，用relu激活\n",
        "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
        "    layers.Dense(units=3, activation='relu'),\n",
        "    # 線性輸出層\n",
        "    layers.Dense(units=1),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7780da59",
      "metadata": {
        "papermill": {
          "duration": 0.003589,
          "end_time": "2021-11-09T00:10:58.105935",
          "exception": false,
          "start_time": "2021-11-09T00:10:58.102346",
          "status": "completed"
        },
        "tags": [],
        "id": "7780da59"
      },
      "source": [
        "確保將所有層一起傳遞到一個列表中，例如 `[layer, layer, layer, ...]`，而不是作為單獨的參數。 要將激活函數添加到層，只需在 `activation` 參數中給出其名稱。\n",
        "\n",
        "# EXERCISE\n",
        "\n",
        "現在，[**為 *Concrete* 數據集創建一個深度神經網絡**](https://www.kaggle.com/kernels/fork/11887344)。"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "formats": "ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 17.556986,
      "end_time": "2021-11-09T00:11:00.995854",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-11-09T00:10:43.438868",
      "version": "2.3.3"
    },
    "colab": {
      "name": "Intro to Deep Learning - L2- deep-neural-networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}